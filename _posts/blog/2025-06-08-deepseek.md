---
layout: post
categories: [blog]
description: > 
  DeepSeek-R1 training pipeline
image: 
  path: https://github.com/user-attachments/assets/f68f9987-ce2b-4423-979f-8ceb88b3f07e
---

# DeepSeek-V3 and DeepSeek R1: A Deep Dive into Scalable Reasoning AI

## 1. DeepSeek-V3: Foundation Model with Mixture-of-Experts
**DeepSeek-V3** is a large language model built using the **Mixture-of-Experts (MoE)** architecture, a technique designed to improve both computational efficiency and specialization within transformer-based models.

### What is Mixture-of-Experts (MoE)?
Instead of using one dense neural network to process all inputs, MoE divides the model into several smaller sub-networks, known as *experts*. Each expert specializes in different aspects of language, such as syntax, semantics, or domain-specific tasks. A lightweight gating network (or *router*) decides which experts to activate for each input token—typically selecting only a few (e.g., 2 out of 64) rather than using all of them. This leads to what is called **sparse activation**, enabling massive models to scale in size without linearly increasing compute cost.

In DeepSeek-V3, MoE is applied to **every transformer layer**, replacing the standard feedforward dense layers. Only a subset of experts is activated per forward pass, which drastically reduces the computation required while still enabling specialization. However, MoE architectures come with their own challenges:

- **Expert underutilization (routing collapse):** The router may overfit to always selecting a small subset of experts, wasting others.
- **Difficulty in specialization:** Without explicit incentives, it's hard for experts to meaningfully specialize.

### DeepSeek’s Solution to MoE Challenges
DeepSeek addresses these problems by:

- Introducing **shared experts** that are always active. These handle general-purpose knowledge and stabilize training.
- Keeping a **large number of non-shared experts**, allowing them to specialize in specific types of inputs.
- Employing techniques to **encourage diverse expert usage**, thus avoiding routing collapse.

This architecture leads to what the paper refers to as **activated parameters**—only a fraction of the total parameters are used per forward pass, providing scalability benefits.

### FP8 Precision and Training Efficiency
Due to limited GPU memory, DeepSeek-V3 was trained using mostly **FP8 (8-bit floating point)** precision. They developed a **custom mixed-precision training framework**, using FP8 for most compute-heavy operations while retaining higher precision (e.g., FP16/FP32) for numerically sensitive parts. This strategy significantly boosts training efficiency without a major sacrifice in model performance.

---

## 2. DeepSeek R1: Reasoning-Centric Post-Training
Building upon DeepSeek-V3, the team developed **DeepSeek R1**, a family of models focused on **reasoning**, **readability**, and **alignment with human preferences**. The R1 development pipeline combines supervised fine-tuning, reinforcement learning, and careful dataset curation.

### Step 1: GPRO – Reasoning-Oriented Reinforcement Learning
The first stage of post-training applies **GPRO** (Generalized Proximal Reinforcement Optimization), a reinforcement learning approach tailored for reasoning. This phase yields the **R1-Zero** model.

Key observations from this phase:

- Accuracy improves over the base V3 model.
- Output length increases, suggesting deeper reasoning per response.
- The model generates structured outputs using `<think>` and `</think>` tags to reflect its internal chain-of-thought (CoT).
- DeepSeek uniquely exposes this reasoning process to users, unlike models like ChatGPT where the reasoning steps are hidden.

However, **R1-Zero** also exhibited flaws:

- Mixed-language responses.
- Poor readability and coherence in some outputs.

### Step 2: Supervised Fine-Tuning (SFT) with Cold Start CoT Data
To address these issues, DeepSeek performs a second round of **SFT**, starting with long-form CoT data ("cold start") to teach the model how to structure reasoning in natural language more effectively.

This is followed by additional **GPRO training** with a **CoT language consistency reward**, designed to encourage clear and well-structured reasoning outputs. Although there is a slight drop in raw performance metrics, the outputs become more **aligned with human preferences** in terms of readability and clarity.

### Step 3: Data Curation and Generation
DeepSeek then uses the above models to **generate new training data** via rejection sampling and filtering:

- They curate ~600K **reasoning-centric samples**, selecting only outputs that pass a quality filter (e.g., no language mixing, short responses, or unreadable code blocks).
- They employ a **generative reward model**, using DeepSeek-V3 to judge the quality of generated samples by comparing outputs to ground-truth answers.

### Step 4: Expanding to Non-Reasoning Tasks
In addition to reasoning, DeepSeek R1 includes ~200K **non-reasoning examples**, such as:

- Factual Q&A
- Self-reflection or self-cognition tasks
- Translation
- Simple conversations

For some of these tasks, a CoT is generated before answering. For very simple prompts (e.g., “hello”), CoT is omitted entirely.

In total, DeepSeek collects ~800K curated samples, which are used to fine-tune **DeepSeek-V3-Base** for two epochs, yielding the final **DeepSeek R1** model.

### Final RL Phase: Helpfulness and Harmlessness
In the final reinforcement learning stage, DeepSeek adds two critical alignment rewards:

- **Helpfulness**: Promotes useful and informative answers.
- **Harmlessness**: Ensures that responses are safe and non-toxic.

---

## DeepSeek R1 Distill: Cross-Model Generalization
Interestingly, DeepSeek also uses its curated dataset to **distill knowledge into other models**, including:

- **Qwen2.5-Math-7B**
- **Qwen2.5-14B**
- **LLaMA 3.1-8B**

These distilled variants—branded under **DeepSeek R1 Distill**—inherit the reasoning capabilities and alignment from the DeepSeek pipeline, extending its influence to other model families.

---

## Conclusion: Competitive with GPT-4o and Claude 3.5 Sonnet
Through a combination of Mixture-of-Experts scaling, reasoning-aware reinforcement learning, and careful data curation, DeepSeek has built a powerful and efficient AI system. Both **DeepSeek-V3** and **DeepSeek R1** demonstrate:

- Strong reasoning ability
- High interpretability via exposed CoT
- Efficient use of compute
- Competitive performance with state-of-the-art models like **GPT-4o** and **Claude 3.5 Sonnet**

With their open research approach and model releases, DeepSeek contributes significantly to the push for transparent and scalable AI development.

### References and Further Reading
- DeepSeek R1 Theory Overview, GRPO + RL + SFT by Deep Learning with Yacine: [YouTube](https://www.youtube.com/watch?v=QdEuh2UVbu0)
- How Did They Do It? DeepSeek V3 and R1 Explained by No Hype AI: [YouTube](https://www.youtube.com/watch?v=fTjPEE0fk-U)
